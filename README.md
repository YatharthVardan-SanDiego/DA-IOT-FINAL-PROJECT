# DA-IOT-FINAL-PROJECT

**Course:** AAI-530 — Data Analytics and the Internet of Things
**Group:** AAI-530 Group 12
**Institution:** University of San Diego
**Dataset:** RT-IoT2022 (UCI ML Repository ID 942)

---

## Project Overview

This project applies machine learning to IoT network security using the **RT-IoT2022** dataset — a real-world network traffic dataset collected from a live IoT infrastructure combining smart home and industrial devices with simulated cyberattack scenarios.

The dataset contains **123,117 labeled network flow records** with **83 features** each, capturing traffic from four IoT device types (ThingSpeak-LED, Wipro Smart Bulb, MQTT Temperature Sensor, Amazon Alexa) alongside nine categories of network attacks (SYN flood, ARP poisoning, Nmap scans, Slowloris DDoS, and SSH brute-force).

Two complementary machine learning models are implemented:
- **Model 1** — A deep neural network (DNN) that classifies each network flow as one of 13 attack or normal traffic types.
- **Model 2** — A stacked LSTM network that treats network flows as a time series and predicts the duration of future flows for anomaly detection.

A full IoT system design and APA 7 report are also included.

---

## Repository File Structure

```
DA-IOT-FINAL-PROJECT/
│
├── models/
│   ├── model1_attack_classifier.py        # Model 1: DNN multi-class attack-type classifier
│   ├── model2_flow_duration_lstm.py       # Model 2: LSTM flow duration time series predictor
│   ├── model1_attack_classifier_saved.keras   # (generated) Saved Keras weights — Model 1
│   └── model2_flow_duration_lstm_saved.keras  # (generated) Saved Keras weights — Model 2
│
├── outputs/
│   ├── model1_predictions.csv             # (generated) Test-set predictions from Model 1
│   ├── model1_training_history.csv        # (generated) Per-epoch loss/accuracy for Model 1
│   ├── model2_predictions.csv             # (generated) Test-set predictions from Model 2
│   └── model2_training_history.csv        # (generated) Per-epoch loss/MAE for Model 2
│
├── diagrams/
│   ├── generate_diagram.py                # Script to regenerate the system architecture diagram
│   ├── system_diagram.png                 # IoT system architecture diagram (5-layer design)
│   ├── system_diagram.svg                 # Scalable vector version
│   └── system_diagram.pdf                 # Print-ready PDF version
│
├── docs/
│   ├── report.md                          # Full APA 7 project report
│   └── dataset.md                         # Dataset loading instructions (ucimlrepo)
│
├── .gitignore
└── README.md                              # This file
```

**Folder descriptions:**

| Folder / File | Description |
|---|---|
| `models/` | All ML source code and saved Keras model checkpoints |
| `outputs/` | CSV files generated by training runs (predictions, training history) |
| `diagrams/` | Architecture diagram script and all output image formats (PNG/SVG/PDF) |
| `docs/` | APA 7 project report and dataset reference guide |

---

## Setup and Installation

### Prerequisites

- Python 3.9 or higher
- pip

### Install Dependencies

```bash
pip install ucimlrepo tensorflow scikit-learn pandas numpy matplotlib
```

**Tested versions:**

| Package | Version |
|---|---|
| Python | 3.9+ |
| TensorFlow | 2.13+ |
| scikit-learn | 1.3+ |
| pandas | 2.0+ |
| numpy | 1.24+ |
| matplotlib | 3.7+ |
| ucimlrepo | 0.0.3+ |

> **Note:** TensorFlow 2.x is required. TensorFlow 1.x is not supported. For Apple Silicon (M1/M2/M3) Macs, use `tensorflow-macos` and `tensorflow-metal` instead of `tensorflow`.

**Apple Silicon installation:**
```bash
pip install tensorflow-macos tensorflow-metal scikit-learn pandas numpy matplotlib ucimlrepo
```

### Dataset

The dataset is fetched automatically from the UCI ML Repository at runtime — no manual download required. An internet connection is needed the first time each script runs. The `ucimlrepo` library caches the dataset locally after the first download.

---

## How to Run

### Model 1 — DNN Attack-Type Classifier

```bash
python models/model1_attack_classifier.py
```

**What it does:**
1. Downloads the RT-IoT2022 dataset from UCI (first run only)
2. Preprocesses features: one-hot encoding, median imputation, StandardScaler
3. Builds a 3-hidden-layer DNN (256→128→64→13 classes) from scratch
4. Trains with Adam, class weights, EarlyStopping, and ReduceLROnPlateau
5. Evaluates on 20% held-out test set (accuracy, macro F1, weighted F1, per-class report)
6. Saves `model1_predictions.csv` and `model1_training_history.csv`

**Expected runtime:** 5–15 minutes depending on hardware (GPU recommended).

**Output files:**
- `outputs/model1_predictions.csv` — columns: `true_label`, `predicted_label`, `confidence`, `correct`
- `outputs/model1_training_history.csv` — columns: `epoch`, `loss`, `accuracy`, `val_loss`, `val_accuracy`
- `models/model1_attack_classifier_saved.keras` — saved Keras model (best checkpoint by val_loss)

---

### Model 2 — LSTM Flow Duration Predictor

```bash
python models/model2_flow_duration_lstm.py
```

**What it does:**
1. Downloads the RT-IoT2022 dataset from UCI (first run only)
2. Drops categorical columns; standardises numeric features and `flow_duration` target separately
3. Constructs sliding-window sequences of length 20 (sequential, no shuffle)
4. Builds a stacked LSTM (64→32 units) + Dense regression head from scratch
5. Trains with MSE loss, Adam, EarlyStopping, and ReduceLROnPlateau
6. Applies log1p transform to target; evaluates on the final 20% of sequences (RMSE, MAE, R² in log1p space + MAE in original seconds)
7. Saves `model2_predictions.csv` and `model2_training_history.csv`

**Expected runtime:** 10–25 minutes depending on hardware (GPU recommended).

**Output files:**
- `outputs/model2_predictions.csv` — columns: `true_flow_duration`, `predicted_flow_duration`, `absolute_error`
- `outputs/model2_training_history.csv` — columns: `epoch`, `loss`, `mae`, `val_loss`, `val_mae`
- `models/model2_flow_duration_lstm_saved.keras` — saved Keras model (best checkpoint by val_loss)

---

### System Architecture Diagram

```bash
python diagrams/generate_diagram.py
```

Regenerates `diagrams/system_diagram.png`. Only needed if the diagram file is missing or you want to modify the architecture.

---

## ML Model Descriptions

### Model 1 — DNN Attack-Type Classifier

**Target variable:** `Attack_type` (categorical, 13 classes)
**Task:** Multi-class classification
**Framework:** TensorFlow / Keras (Functional API, no pre-built architectures)

**Architecture:**
```
Input(n_features) → Dense(256)+BN+ReLU+Dropout(0.4)
                  → Dense(128)+BN+ReLU+Dropout(0.3)
                  → Dense(64)+BN+ReLU+Dropout(0.2)
                  → Dense(13)+Softmax
```

**Key design choices:**
- BatchNormalization reduces internal covariate shift and stabilises training
- Decreasing layer width (256→128→64) compresses features progressively
- Class weights address severe imbalance (DOS_SYN_Hping = 76.9% of data)
- L2 regularisation (λ=1e-4) prevents overfitting to dominant classes

**Evaluation metrics:** Accuracy, Macro F1, Weighted F1, per-class precision/recall/F1

**Results:**
| Metric | Value |
|---|---|
| Test Accuracy | 98.79% |
| Macro F1-Score | 0.8495 |
| Weighted F1-Score | 0.9886 |

---

### Model 2 — LSTM Flow Duration Predictor

**Target variable:** `flow_duration` (continuous, seconds)
**Task:** Time series regression — predict next flow's duration from the past 20 flows
**Framework:** TensorFlow / Keras (from scratch)

**Architecture:**
```
Input(20 × n_features) → LSTM(64, return_sequences=True) → Dropout(0.3)
                       → LSTM(32, return_sequences=False) → Dropout(0.2)
                       → Dense(16, ReLU)
                       → Dense(1)
```

**Key design choices:**
- Stacked LSTM captures both local patterns (Layer 1) and long-range dependencies (Layer 2)
- Chronological train/test split preserves temporal ordering (no future leakage)
- Separate StandardScaler for target enables metric reporting in original units
- MSE loss penalises large deviations, helping detect anomalously long/short flows

**Evaluation metrics:** RMSE, MAE (log1p space and original seconds), R² (log1p space)

**Results:**
| Metric | Value |
|---|---|
| Test RMSE (log1p) | 0.6274 |
| Test MAE (log1p) | 0.2738 |
| Test R² (log1p) | −48.87 |
| Test MAE (seconds) | 0.7940 s |

> Note: The negative R² reflects class-contiguous ordering in the dataset rather than a continuous clock. See `report.md` for full discussion.

---

## Team — AAI-530 Group 12

- Lokesh Upputri
- Yatharth Vardan
- Senthil Arasu T

---

## Citation

If referencing the dataset:

> Sharmila Kinnal, B., Khanum, F., Manzoor, U., Akhter, N., & Bhavani, R. (2023). *RT-IoT2022* [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5P338
